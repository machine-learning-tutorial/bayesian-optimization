{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Bayesian Optimization\n",
    "\n",
    "In this tutorial notebook we'll implement all the basic components of Bayesian optimization (BO), and see how to use BO for some sample functions.\n",
    "\n",
    "## 0. Glossary and General Introduction\n",
    "\n",
    "### 0.1 Bayes' Theorem\n",
    "\n",
    "First, let's start with the absolute basics of Bayesian inference, the Bayes' theorem\n",
    "    $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "- $P(A|B)$ is the _posterior probablity_ of event $A$ given that event B is observed.\n",
    "- $P(B|A)$ is the _likelihood function_ of A given B; it is also the conditional probablity of observing $B$ given $A$.\n",
    "- $P(A), P(B)$ are the _prior probabilities_ of observing $A$ and $B$, also known as _marginal probability_\n",
    "\n",
    "For most applications, the observed event $B$ is already fixed and we can consider $P(B)$ simply as a constant. Thus the Bayes' theorem reads\n",
    "    $$P(A|B) \\propto P(B|A) P(A) $$,\n",
    "i.e. the __posterior probability__ is proportional to __priors__ times __likelihood__.\n",
    "\n",
    "### 0.2 Gaussian Process\n",
    "\n",
    "A Gaussian process (GP) is a stochastic process (a joint distribution of infinitely many random variables), which can be used as a probablistic model (surrogate model) for the objective function in regression and classification tasks.\n",
    "\n",
    "A GP can be fully described by it's mean $\\mu$ and covariance function $k(\\cdot,\\cdot)$\n",
    "    $$f(x) \\sim \\mathcal{GP}(\\mu(x), k(x,x'))$$,\n",
    "where $x$ and $x'$ are points in the input space. \n",
    "\n",
    "- __Gaussian process regression (GPR)__, also formerly known as _kriging_: a method to interpolate a unkown objective function.\n",
    "- __Prior mean__ $\\mu(x)$: basic building block of GP; prior belief on the averaged objective function values, usually set to a constant if the function behaivor is unknown.\n",
    "- __Kernel__ $k(\\cdot,\\cdot)$, also known as the _covariance function_ $cov(\\cdot,\\cdot)$: basic building block of GP; prior belief on the characteristics of the unkown function.\n",
    "\n",
    "_Note: in GP regression, one usually understands $x'$ as the observed points, and $x$ as the continous variable_\n",
    "\n",
    "#### 0.2.1 Common Kernels used in GP\n",
    "\n",
    "Some of the commonly used kernels are listed below. They can also be combined to build more complex kernels representing the underlying physics of the objective function.\n",
    "\n",
    "For simplicity reason, we define $d_{x,x'}$ as the Euclidean distance between the two points\n",
    "    $$ d_{x,x'} := ||x - x'||_2 $$\n",
    "\n",
    "- Linear: $k_\\mathrm{L}(x,x') = x^\\intercal x'$. A special case that one might not often use (non-stationary kernel), this reduces GP essentially back to Bayesian linear regression.\n",
    "- __White Gaussian noise__: $k_\\mathrm{n}(x,x')=\\sigma_{n}^2 \\delta_{x,x'}$ , i.e. a diagnal noise term with Gaussian noise $\\sigma_{n}^2$.\n",
    "- __Radial basis function (RBF)__, also known as squared exponential (SE): $k_{\\mathrm{RBF}} (x,x') = \\exp(-\\frac{d_{x,x'}^2}{2 l^2})$ , where $l$ is the lengthscale, see below. This resembles a normal Gaussian distribution. It is more or less _the default_ choice of kernel for GPs if one does not have a special assumption on the objective function.\n",
    "- Rational quadratic (RQ): $k_\\mathrm{RQ}=\\left( 1 + \\frac{d_{x,x'}}{2\\alpha l^2} \\right)^{-\\alpha}$, equivalent of sum of RBF kernels with many different lengthscales. Approaches RBF when $\\alpha \\to \\infty$\n",
    "- Matérn: $k_\\mathrm{Matern} (x,x') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu d_{x,x'}} / l \\right)^\\nu K_\\nu \\left( \\sqrt{2\\nu d_{x,x'}} / l \\right) $, where $\\Gamma$ is the gamma function, $K_\\nu$ the modified Bessel function and $\\nu$ the parameter of the Matérn kernel. Common choices are $\\nu=\\frac{3}{2},\\frac{5}{2},...$ for once and twice differentiable functions.\n",
    "- Ornstein-Uhlenbeck: $k_\\mathrm{OU}(x,x')=\\exp(-d_{x,x'}/l)$\n",
    "\n",
    "\n",
    "#### 0.2.2 GP Hyperparamters \n",
    "\n",
    "The characteristics of GP, or its ability to approximate the unknown function are dependent both on __the choice of the covariance function__ and the __values of the hyperparamters__. The hyperparamters are usually either choosen manually based on the physics, or obtained from the maximum likelihood fit (log-likelihood fit, maximum a posteriori fit) during the optimization.\n",
    "\n",
    "- __Lengthscale__ $l$: controls the scaling of different input dimensions, i.e. how fast the objective function is expected to change from observed points.\n",
    "- __Noise variance__  $\\sigma_\\mathrm{n}^2$: magnitude of the noise in the observed values.\n",
    "- __Signal variance__ $\\sigma^2$: a scaling factor to be multiplied to the kernel function, it is essentially equivalent to normalize/scale the objective function.\n",
    "\n",
    "\n",
    "### 0.3 Bayesian Optimization\n",
    "\n",
    "Bayesian optimization is a sequential algorithm to globally optimize an unknown function.\n",
    "\n",
    "Below are some common terms used in the BO field, some of which are often used interchangeably.\n",
    "\n",
    "- __Acquisition function__ $\\alpha$: is built on the GP posterior, controls the behavior of optimization. For the standard verison of BO, the next sample point is chosen at $\\mathrm{argmax}(x)$. In this tutorial we will introduce two widely used acquisition functions: the expected improvment (EI) and the upper confidence bound (UCB).\n",
    "- __Objective__, metric, or target function: a unknown (black-box) function, for which the value is to be optimized (here: maximization).\n",
    "- __Search space__, bounds, or optimization range: A (continous) parameter space where the input parameters are allowed to be varied in the optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, DotProduct, WhiteKernel, Matern, RationalQuadratic\n",
    "from sklearn.datasets import make_friedman2\n",
    "from skopt import gp_minimize\n",
    "from scipy.optimize import minimize\n",
    "from utils.gp_helper import plot_gpr_samples, plot_gp, plot_gp_with_acq, plot_bo_result, AcqUCB, AcqEI\n",
    "\n",
    "# Suppress some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Gaussian Process\n",
    "\n",
    "Here we will use the [scikit-learn](https://scikit-learn.org/stable/modules/gaussian_process.html) to build a Gaussian process model.\n",
    "In the end of this notebook you will also find a incomplete set of other commonly used GP and BO libraries.\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a RBF kernel (covariance function)\n",
    "k_rbf = 1 * RBF(length_scale=1.0, length_scale_bounds='fixed')  # with a 'fixed' length_scale\n",
    "# Initialize a GP model\n",
    "gp_with_rbf = GaussianProcessRegressor(kernel=k_rbf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sample some functions from the prior distribution (which is essentially a constant function with some standard deviation).\n",
    "\n",
    "\n",
    "_Remark_: The actual function calls for GP prediction in `plot_gpr_samples`\n",
    "\n",
    "```python\n",
    "# Predict GP mean and std\n",
    "y_mean, y_std = gpr_model.predict(x, return_std=True)\n",
    "# Sample functions from the GP distribution\n",
    "y_samples = gpr_model.sample_y(x, n_samples)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,5,200)\n",
    "fig, ax = plt.subplots()\n",
    "plot_gpr_samples(gp_with_rbf, ax=ax, x=x, n_samples=5, random_state=0)\n",
    "ax.set_ylim(-3,3)\n",
    "ax.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some GPs with other kernels for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Feel free to also modify the length scale parameter to see the effects]\n",
    "k_rq = 1 * RationalQuadratic(length_scale=1.0, length_scale_bounds='fixed', alpha=1)\n",
    "k_matern32 = Matern(length_scale=1.0, length_scale_bounds='fixed', nu=3/2)\n",
    "k_matern52 = Matern(length_scale=1.0, length_scale_bounds='fixed', nu=5/2)\n",
    "\n",
    "gp_with_rq = GaussianProcessRegressor(kernel=k_rq)\n",
    "gp_with_matern32 = GaussianProcessRegressor(kernel=k_matern32)\n",
    "gp_with_matern52 = GaussianProcessRegressor(kernel=k_matern52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,5,200)\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,10))\n",
    "plt_titles = [\"RBF\",\"RationalQuadratic\",\"Matern3/2\",\"Matern5/2\"]\n",
    "plot_gpr_samples(gp_with_rbf, ax=axes[0,0], x=x, n_samples=5)\n",
    "plot_gpr_samples(gp_with_rq, ax=axes[0,1], x=x, n_samples=5)\n",
    "plot_gpr_samples(gp_with_matern32, ax=axes[1,0], x=x, n_samples=5)\n",
    "plot_gpr_samples(gp_with_matern52, ax=axes[1,1], x=x, n_samples=5)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.set_ylim(-3,3)\n",
    "    ax.set_title(plt_titles[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clearly see that the sampled functions from RBF and RQ kernels are more smooth than the Matern ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define an Unkown Target Function\n",
    "\n",
    "We'll define a target function with multiple local optima as the objective function for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_target(x, noise=0.1, rng = np.random.default_rng()):\n",
    "    f = (-1.4 + 3.0 * x) * np.sin(18.0 * x)\n",
    "    return f + rng.random(x.shape) * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize it\n",
    "xlow, xhigh = 0, 1\n",
    "x = np.linspace(xlow,xhigh,200)\n",
    "y = f_target(x, noise=0)\n",
    "x_samples = np.array([0.13, 0.93, 0.25, 0.55, 0.71, 0.83])   # for now, just manually define some observations for reproducibility :)\n",
    "\n",
    "# Other wide one can define a seeded random generator for reproducibility\n",
    "rng = np.random.default_rng(1)\n",
    "# x_samples = rng.uniform(low=xlow,high=xhigh,size=6)\n",
    "y_samples = f_target(x_samples, noise=0.1, rng=rng)\n",
    "\n",
    "plt.plot(x,y,label='True f')\n",
    "plt.plot(x_samples,y_samples,\"*\",label=\"Noisy Samples\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the GP posterior distribution looks like with some observed points.\n",
    "\n",
    "Figure legends:\n",
    "- black: GP posterior mean function\n",
    "- gray shadow: $\\pm 1$ std. from GP posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how GP is fitted to the observations\n",
    "kernel = RBF(length_scale=0.1, length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.1**2, noise_level_bounds='fixed')\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "\n",
    "nstep=1\n",
    "# fit GP to the function and plot posterior\n",
    "fig,axes = plt.subplots(2,3,figsize=(18,8))\n",
    "for nstep in range(6):\n",
    "    ax = axes.flatten()[nstep]\n",
    "    gpr.fit(x_samples.reshape(-1,1)[:nstep+1],y_samples[:nstep+1])\n",
    "    ax.set_title(f\"Step {nstep+1}\")\n",
    "    plot_gp(gpr, x, y, x_samples[:nstep+1], y_samples[:nstep+1], ax=ax)\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.legend(ncol=2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feel free to change the hyperparameters, i.e. length_scale, variance, and noise variance, below to see the difference in resulted GP posteriors__\n",
    "\n",
    "For example, one can vary from small to large lengthscales like `[0.01, 0.1, 0.5]`, or enable automatic lengthscale fitting within a specified range via setting `length_scale_bounds=[1e-3,1e2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's redefine GP model with \n",
    "\n",
    "kernel = 1 * RBF(length_scale=0.1, length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.1**2, noise_level_bounds='fixed')\n",
    "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "\n",
    "# Visualize the GP fitted to the target function\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "ax1, ax2 = axes\n",
    "\n",
    "# plot GP prior\n",
    "ax1.set_title(\"Prior\")\n",
    "plot_gp(gpr, x, y, x_samples, y_samples, ax=ax1)\n",
    "\n",
    "# fit GP to the function and plot posterior\n",
    "gpr.fit(x_samples.reshape(-1,1),y_samples)\n",
    "ax2.set_title(\"Posterior\")\n",
    "plot_gp(gpr, x, y, x_samples, y_samples, ax=ax2)\n",
    "ax1.set_ylim(-1.5,1.5)\n",
    "ax2.set_ylim(-1.5,1.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.legend(ncol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximization of the acquisition function\n",
    "\n",
    "With the GP model approximating the objective function, we can build an acquisition function to assess what will be the most valuable point to sample at next step.\n",
    "\n",
    "Here we will implement two acquisition functions: \n",
    "\n",
    "The __expected improvement (EI)__\n",
    "$$ \\begin{split}   \n",
    "\\alpha_{\\text{EI}} (x)  &= \\mathbb{E} [ \\max (\\mu (x)-(f_{\\text{best}}+\\xi),0)  ] \\\\  \n",
    "&= (\\mu(x)-(f_{\\text{best}}+\\xi)) \\Phi (Z) + \\sigma (x)\\phi (Z) \n",
    "\\end{split}, \n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{split}\n",
    "         Z  & = \\frac{\\mu(x)-(f_{\\text{best}}+\\xi)}{\\sigma(x)}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The EI acquisition calculates the expectation value of improvement of the function value at a to-be-sampled point $x$ compared to the best observation so far $f_{\\text{best}}$.\n",
    "\n",
    "Another is the __upper confidence bound (UCB)__\n",
    "$$\n",
    "    \\alpha_{\\text{UCB}} (x) = \\mu(x) + \\kappa \\sigma (x),\n",
    "$$\n",
    "which calculates the posterior mean $\\mu$ plus a factor times the standard deviation $\\sigma$ at a to-be-sampled point $x$.\n",
    "\n",
    "See `utils/gp_helper.py` for the implementation of the acquisition fucntions.\n",
    "\n",
    "Other popular acquisition functions are: entropy search, knoledge gradient, etc...\n",
    "\n",
    "__Exploration-exploitation tradeoff__: the behaviour of the acquisition fucntions are controlled via the hyperparameters $\\xi$ and $\\kappa$. Larger values lead to more exploration and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a EI acquisition\n",
    "acq_EI = AcqEI(xi=0.)  # feel free to change the value of xi to see the different behaviour of EI\n",
    "\n",
    "# Calculate the acquisition function\n",
    "y_acq = acq_EI.get_acq(x.reshape(-1,1), gpr)\n",
    "\n",
    "# Visualize the acquisition functions \n",
    "\n",
    "fig, axes = plt.subplots(2,1,figsize=(5,4),gridspec_kw={\"height_ratios\": [2,1]})\n",
    "ax1, ax2 = axes\n",
    "ax2.set_ylabel('EI')\n",
    "\n",
    "plot_gp_with_acq(gpr, x, y, x_samples, y_samples, y_acq, axes=axes, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UCB acquisition\n",
    "acq_UCB = AcqUCB(k=2)  # feel free to change the value of xi to see the different behaviour of EI\n",
    "\n",
    "# Calculate the acquisition function\n",
    "y_acq = acq_UCB.get_acq(x.reshape(-1,1), gpr)\n",
    "\n",
    "# Visualize the acquisition functions \n",
    "\n",
    "fig, axes = plt.subplots(2,1,figsize=(5,4),gridspec_kw={\"height_ratios\": [2,1]})\n",
    "ax1, ax2 = axes\n",
    "ax2.set_ylabel('UCB')\n",
    "\n",
    "plot_gp_with_acq(gpr, x, y, x_samples, y_samples, y_acq, axes=axes, fig=fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Maximize the Target Function\n",
    "\n",
    "_Note_: Here the code is used to maximize a unknown target fucntion. For minimization tasks, one can simply multiply -1 to the target function.\n",
    "\n",
    "Some helper functions are imported to plot the progress of BO, see `utils/gp_helper.py`\n",
    "\n",
    "The most simple form of Bayesian optimization can be divided in following steps:\n",
    "\n",
    "1. Initialize GP model $\\mathcal{GP}(\\mu, \\sigma)$\n",
    "2. Build acquisition function $\\alpha$\n",
    "3. Sample next point $x_i$ of target function $f$ at $\\text{argmax}(\\alpha)$\n",
    "4. Observe $y_i = f(x_{i})$ and refit GP model.\n",
    "\n",
    "\n",
    "\n",
    "First, let's see some BO steps in action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the sample points for reproducibility\n",
    "rng = np.random.default_rng(1)\n",
    "X = np.array([0.13, 0.93, 0.25, 0.55, 0.71,])\n",
    "Y = f_target(X, noise=0.1, rng=rng)\n",
    "# define a GP with RBF kernel\n",
    "kernel = RBF(length_scale=0.1, length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.1**2, noise_level_bounds='fixed')\n",
    "gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "# define an acquisition function \n",
    "myacq = AcqUCB(k=2)\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "gs = GridSpec(2, 3, figure=fig)\n",
    "\n",
    "for i in range(6):\n",
    "    # Fit GP model to observed data\n",
    "    gpr.fit(X.reshape(-1,1), Y.reshape(-1,1))\n",
    "    # Calculate the acquisition function\n",
    "    y_acq = myacq.get_acq(x.reshape(-1,1), gpr)\n",
    "\n",
    "    # Visualize the acquisition functions \n",
    "    inner_grid = gs[np.unravel_index(i, [2,3])].subgridspec(2, 1, height_ratios=[2,1])\n",
    "    axes = inner_grid.subplots()\n",
    "    axes[0].set_title(f\"Step {i+1}\")\n",
    "    axes[0].set_ylim(-1.5,1.5)\n",
    "    axes[1].set_ylabel(r\"$\\alpha$\")\n",
    "    plot_gp_with_acq(gpr, x, y, X, Y, y_acq, axes=axes, fig=fig, legend=False)\n",
    "    # Append new data\n",
    "    x_next = x[np.argmax(y_acq)]\n",
    "    y_next = f_target(x_next, noise=0.1, rng=rng)\n",
    "    X = np.vstack([X.reshape(-1,1), x_next.reshape(-1,1)])\n",
    "    Y = np.vstack([Y.reshape(-1,1), y_next.reshape(-1,1)])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we bring the components together and define a optimizer function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Bayesian optimizer  \n",
    "\n",
    "def bayesian_optimize(\n",
    "    gpmodel: GaussianProcessRegressor, \n",
    "    acquisition, \n",
    "    target_func, \n",
    "    init_points=None, \n",
    "    steps=50, \n",
    "    bounds=None, \n",
    "    xdim=1, \n",
    "    rng=np.random.default_rng()):\n",
    "    \n",
    "    # for logging purpose\n",
    "    history = {\n",
    "        \"X_init\": [],\n",
    "        \"Y_init\": [],\n",
    "        \"X\": [],\n",
    "        \"Y\": [],\n",
    "        \"X_best\": [],\n",
    "        \"Y_best\": [],\n",
    "        }\n",
    "\n",
    "    # Initial samples to start with \n",
    "    if init_points is None:\n",
    "        init_points = rng.uniform(low=bounds[:, 0], high=bounds[:, 1], size=(3, xdim))\n",
    "    y_init = []\n",
    "    for xi in init_points:\n",
    "        y_init.append(target_func(xi))\n",
    "    y_init = np.array(y_init)\n",
    "    history[\"X_init\"] = init_points\n",
    "    history[\"Y_init\"] = y_init\n",
    "    n_init = init_points.shape[0]\n",
    "\n",
    "    X = init_points.reshape(-1,xdim)\n",
    "    Y = y_init.reshape(-1,1)\n",
    "    \n",
    "    # Actual optimization step\n",
    "    for _ in range(steps):\n",
    "        # fit gp model\n",
    "        gpmodel.fit(X, Y)\n",
    "        # maximize acquisition\n",
    "        x_next = acquisition.suggest_next_sample(gpmodel, bounds=bounds)\n",
    "        # sample at argmax(acquisition)\n",
    "        y_next = target_func(x_next)\n",
    "        # augment data\n",
    "        X = np.vstack([X, x_next.reshape(-1,xdim)])\n",
    "        Y = np.vstack([Y, y_next.reshape(-1,1)])\n",
    "    \n",
    "    # log process\n",
    "    history[\"X\"] = X[n_init:]\n",
    "    history[\"Y\"] = Y[n_init:]\n",
    "    i_best = Y.argmax()\n",
    "    history[\"X_best\"] = X[i_best]\n",
    "    history[\"Y_best\"] = Y[i_best]\n",
    "    \n",
    "    return gpmodel, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Start Optimization [Feel free to play around with the parameters in this block and observe effects]\n",
    "\n",
    "# define a GP model\n",
    "kernel = 1 * RBF(length_scale=0.5, length_scale_bounds=[1e-2,1]) + WhiteKernel(noise_level=0.1) # allow automatic determination of length-scale\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "# define a Acquisition function\n",
    "myacq = AcqEI(xi=0.001)\n",
    "\n",
    "# start BO\n",
    "bounds = np.array([[0,1]])\n",
    "nsteps=30\n",
    "gpr, history = bayesian_optimize(gpr, myacq, target_func=f_target, steps=nsteps, bounds=bounds, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "fig, ax = plt.subplots()\n",
    "x_fine = np.linspace(0,1,500)\n",
    "y_fine = f_target(x_fine,noise=0)\n",
    "ymax = np.ones(nsteps)*y_fine.max()\n",
    "ax.plot(ymax, ls='--',color='gray', label='True maximum')\n",
    "ax.fill_between(np.arange(nsteps), ymax-0.1, ymax+0.1, alpha=0.3, color='gray')\n",
    "ax.plot(history[\"Y\"], label=\"BO result\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance benchmark (Optional Module)\n",
    "\n",
    "Against some \"classical\" algorithms:\n",
    "- Nelder-Mead\n",
    "- Random search\n",
    "- ...\n",
    "\n",
    "### Define a more complex function\n",
    "\n",
    "Let's use the Ackley function:\n",
    "$$\n",
    "    f(x_1,x_2) = -20 \\exp \\left[-0.2 \\sqrt{0.5(x_1^2+x_2^2)} \\right] - \\exp \\left[0.5(\\cos(2\\pi x_1) + \\cos(2\\pi x_2)) \\right] + e + 20\n",
    "$$\n",
    "\n",
    "![](img/ackley.png)\n",
    "\n",
    "_Feel free to insert your favorite benchmark function here_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define more complex functions\n",
    "def f_ackley(x: np.ndarray, yhist = None) -> np.ndarray:\n",
    "    \"\"\"func_log: used to track the optimization progress\"\"\"\n",
    "    assert x.shape[1] ==2\n",
    "    y = -20 * np.exp(-0.2 * np.sqrt(0.5 * np.sum(np.square(x), axis=1))) \n",
    "    y -= np.exp(0.5*(np.cos(2*np.pi*x[:,0]) + np.cos(2*np.pi*x[:,1])))\n",
    "    y += np.e + 20\n",
    "    y *= -1\n",
    "    if yhist is not None:\n",
    "        yhist.append(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start optimization with different algorithms\n",
    "\n",
    "_Note: running the cells below takes ~10 min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_ackley = np.array([[-5,5],[-5,5]])\n",
    "n_restart = 5  # try 5 times for each algorithm\n",
    "maxiter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nelder Mead\n",
    "nm_hist = [0] * n_restart\n",
    "for i in range(n_restart):\n",
    "    rng = np.random.default_rng(i)\n",
    "    x0 = rng.uniform(low=bounds_ackley[:,0], high=bounds_ackley[:,1])\n",
    "    nm_hist[i] = []\n",
    "    res = minimize(\n",
    "        lambda x: -1 * float(f_ackley(np.array(x).reshape(-1,2), nm_hist[i])),\n",
    "        x0 = x0,\n",
    "        bounds=bounds_ackley,\n",
    "        options={'maxfev': maxiter, 'xatol': 1e-10},\n",
    "        method='Nelder-Mead'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# BO our version: more exploitation\n",
    "bo_hist_exploit = [0] * n_restart\n",
    "kernel = 1 * RBF(length_scale=0.1, length_scale_bounds=[1e-2,1]) #\n",
    "for i in range(n_restart):\n",
    "    rng = np.random.default_rng(i)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "    myacq = AcqEI(xi=0.01)\n",
    "    bo_hist_exploit[i] = []\n",
    "    _, _ = bayesian_optimize(\n",
    "        gpr, myacq, target_func=lambda x: f_ackley(np.array(x).reshape(-1,2), bo_hist_exploit[i]), steps=maxiter, bounds=bounds_ackley, rng=rng, xdim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BO our version: more exploration\n",
    "bo_hist_explore = [0] * n_restart\n",
    "kernel = 1 * RBF(length_scale=0.1, length_scale_bounds=[1e-2,1]) #\n",
    "for i in range(n_restart):\n",
    "    rng = np.random.default_rng(i)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "    myacq = AcqEI(xi=0.1)\n",
    "    bo_hist_explore[i] = []\n",
    "    _, _ = bayesian_optimize(\n",
    "        gpr, myacq, target_func=lambda x: f_ackley(np.array(x).reshape(-1,2), bo_hist_explore[i]), steps=maxiter, bounds=bounds_ackley, rng=rng, xdim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture  \n",
    "# suppress output\n",
    "\n",
    "# Default sklearn version of BO\n",
    "skopt_hist = [0] * n_restart\n",
    "for i in range(n_restart):\n",
    "    skopt_hist[i] = []\n",
    "    res = gp_minimize(lambda x: -1 * float(f_ackley(np.array(x).reshape(-1,2), skopt_hist[i])),dimensions=bounds_ackley,n_calls=maxiter, random_state=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure random search, as a baseline\n",
    "random_hist = np.zeros((n_restart, maxiter))\n",
    "x_random = np.random.uniform(low=bounds_ackley[:,0], high=bounds_ackley[:,1], size=(n_restart*maxiter,2))\n",
    "y_random = f_ackley(x=x_random)\n",
    "random_hist = y_random.reshape((n_restart, maxiter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig,ax = plt.subplots(figsize=(5,3))\n",
    "plot_bo_result(np.squeeze(nm_hist), ax, label='Nelder-Mead')\n",
    "plot_bo_result(np.squeeze(skopt_hist), ax, label='scikit-BO')\n",
    "plot_bo_result(np.squeeze(bo_hist_explore), ax, label='BO explore')\n",
    "plot_bo_result(np.squeeze(bo_hist_exploit), ax, label='BO exploit')\n",
    "plot_bo_result(np.squeeze(random_hist), ax, label='Random search')\n",
    "\n",
    "ax.set_xlim(-2,100)\n",
    "ax.set_ylabel(\"Best observed value\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "\n",
    "\n",
    "fig.subplots_adjust(0,0,0.8,0.85,hspace=0.1)\n",
    "fig.legend(bbox_to_anchor = (0.95,0.0,0.2,0.5))\n",
    "#fig.savefig(\"img/benchmark_ackley.png\",dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the result, incase the optimization is not finished\n",
    "![benchmark_ackley](img/benchmark_ackley.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Hopefully you have learned something about BO, if you want to try it yourself afterwards, below are some interesting resources.\n",
    "\n",
    "\n",
    "### Publication: Various applications of BO in accelerator physics\n",
    "\n",
    "- LCLS: [Bayesian optimization of FEL performance at LCLS](https://accelconf.web.cern.ch/ipac2016/doi/JACoW-IPAC2016-WEPOW055.html), [Bayesian Optimization of a Free-Electron Laser](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.124.124801) FEL performance tuning with quadrupoles\n",
    "- LUX: [Bayesian Optimization of a Laser-Plasma Accelerator](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.104801) LPA tuning to improve bunch quality with laser energy, focus position, and gas flows.\n",
    "- Central Laser Facility, Rutherfold UK: [Automation and control of laser wakefield accelerators using Bayesian optimization](https://www.nature.com/articles/s41467-020-20245-6), LWFA performance tuning with different objective functions\n",
    "- PSI, SwissFEL: [Tuning particle accelerators with safety constraints using Bayesian optimization](https://doi.org/10.1103/PhysRevAccelBeams.25.062802) [Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces](https://arxiv.org/abs/1902.03229) : BO with safety contraints to protect the machine\n",
    "- SLAC, ANL: [Multiobjective Bayesian optimization for online accelerator tuning](https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.24.062801), multiobjective optimization for accelerator tuning,[Turn-key constrained parameter space exploration for particle accelerators using Bayesian active learning](https://www.nature.com/articles/s41467-021-25757-3), Bayesian active learning for effcient exploration of the parameter space. [Differentiable Preisach Modeling for Characterization and Optimization of Particle Accelerator Systems with Hysteresis](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.128.204801) hysteris modelling with GP, and application of hysteresis-aware BO.\n",
    "\n",
    "### Books and papers on Bayesian optimization in general\n",
    "\n",
    "- C. E. Rasmussen and C. K.I. Williams, [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/): __the__ classic textbook of Gaussian process.\n",
    "- Eric Brochu, [A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/abs/1012.2599): a comprehensive tutorial on Bayesian optimization with some application cases at that time (2010).\n",
    "- Peter I. Frazier, [A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811): a more recent (2018) tutorial paper covering the most important aspects of BO, and some advanced variants of BO (parallel, multi-fidelity, multi-task).\n",
    "\n",
    "\n",
    "### Bayesian Optimization / Gaussian Process packages in python\n",
    "\n",
    "Below is a incomplete selection of python packages for BO and GP, each with its own strength and drawback.\n",
    "\n",
    "- [scikit-learn Gaussian processes](https://scikit-learn.org/stable/modules/gaussian_process.html#) : recommended for sklearn users, not as powerful as other packages.\n",
    "- [GPyTorch](https://gpytorch.ai/) : a rather new package implemented natively in PyTorch, which makes it very performant. Also comes with a Bayesian optimization package [BOTorch](https://botorch.org/), offering a variety of different optimization methods (mult-objective, parallelization...). Both packages are being actively developed maintained; recommended for PyTorch users.\n",
    "- [GPflow](https://www.gpflow.org/) : a GP package implemented in TensorFlow, it also has a large community and is being actively maintained; The new BO package [Trieste](https://secondmind-labs.github.io/trieste) is built on it.\n",
    "- [GPy](http://sheffieldml.github.io/GPy/) from the Sheffield ML group : A common/classic choice for building GP model, includes a lot of advanced GP variants; However in recent years it is not so actively maintained. It comes with the accompanying Bayesian optimization package [GPyOpt](https://github.com/SheffieldML/GPyOpt), for which the maintainance stoped since 2020.\n",
    "- [Dragonfly] : a open-source BO package; offers also command line tool, easy to use if you are a practitioner. However if one has less freedom to adapt and expand the code.\n",
    "\n",
    "C.f. the [wikipedia page](https://en.wikipedia.org/wiki/Comparison_of_Gaussian_process_software#Comparison_table) for a more inclusive table with GP packages for other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mt-ard-st3-ml-workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6cfeb72334fc509d8f00f181838b192dd92f3c4eaa9af2c9882b974756a1a344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
